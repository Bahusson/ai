{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PANEL GŁÓWNY Numpy ### - Zmienne niezbędne dla wykonania zbioru danych przez Numpy biorą się stąd. \n",
    "# Szczegóły słabiej opisanych - zeszyt kursowy 1\n",
    "\n",
    "observations = 1000  # Ustaw ilość obserwacji (Startowa: 1000)\n",
    "noic = 10            # Szum w bazie przypadków treningowych.\n",
    "noit = 1             # Szum wygenerowany dla funkcji testującej (docelowej).\n",
    "model = [13,7,-12]   # Parametry modelu losowego dla funkcji a*xs + b*xz + c + szum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Tworzenie i składowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.random.uniform(low = -noic, high = noic, size =(observations,vnum)) # Gen. zakr. los. zm. wej.(patrz zeszyt 1)\n",
    "zs = np.random.uniform(-noic,noic,(observations,vnum)) # J.w.- bez opisów bo zbędne. Poz. dla orientacji.\n",
    "\n",
    "generated_inputs = np.column_stack((xs,zs)) # Składanie z pow. matrycy w formacie observationsx2. (Patrz zeszyt 1)\n",
    "\n",
    "noise = np.random.uniform(-noit,noit,(observations,1)) # Gen. zakr. los. (Patrz zeszyt 1)\n",
    "\n",
    "generated_targets = model[0]*xs + model[1]*zs + model[2] + noise # Funkcja symulująca zadane cele. Do niej dąży model. (Patrz zeszyt 1)\n",
    "\n",
    "np.savez('My_Nums', inputs = generated_inputs, targets = generated_targets) # Tworzy plik My_Nums.npz w tym samym miejscu co ten zeszyt. \n",
    "# Zawiera podane matryce w formie ndarray, co dla obsługi TF jest bardzo wygodne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tutaj zaczyna się TensorFlow\n",
    "Uwaga odnośnie używania TF - obiekty TF nie wykonują się same.\n",
    "Trzeba im wydać konkretne polecenie. \n",
    "\n",
    "Jeśli chcesz na to spojrzeć z innej strony, \n",
    "to zeszyt badawczy 2a zawiera opis i przykłady poszczególnych elementów TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Panel Główny TF ### - tutaj będę wyciągał zmienne dla modelu TF,\n",
    "# żeby było jasne co jest do czego. Normalnie wszystkie zmienne,\n",
    "# którymi miałbym potencjalnie poźniej manipulować zbieram dla porządku 1 miejscu.\n",
    "\n",
    "input_size = 2 # Ilość zmiennych wejściowych - tutaj mamy dwie (x,z)\n",
    "output_size = 1 # Nasza funkcja ma jedno wyjście - y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rysujemy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tworzy podstawnik do którego będziemy podstawiać dane wejściowe z naszego zbioru danych w pliku npz.\n",
    "# 'float32' to miara precyzji (32bit) - standard dla Pythona to float64, ale tyle na ogół wystarczy.\n",
    "# W nawiasie kwadratowym znajdują się wymiary podstawianych matryc. 'None' jest w pierwszym nie dlatego,\n",
    "# że nasze dane nie mają pierwszego wymiaru (tj. ilość rekordów), ale że nie musimy tego określać.\n",
    "# To przydatne, bo nie trzeba dzięki temu znać dokładnej ilości obserwacji ani pisać odpowiedniego kodu samemu.\n",
    "# Ważne jedynie, żeby podać ilość zmiennych wejściowych (input_size) i wyjściowych (output_size).\n",
    "inputs = tf.placeholder(tf.float32,[None,input_size])\n",
    "# Podstawnik gdzie podamy funkcji dane testowe (cele) - jest w tym samym kształcie co dane wyjściowe,\n",
    "# więc korzystamy ze zmiennej opisującej ilość zmiennych wyjściowych (output_size).\n",
    "targets = tf.placeholder(tf.float32,[None,output_size])\n",
    "\n",
    "### To były podstawniki. Zauważ, że pomiędzy iteracjami nie zachowują one swojej wartości. Są one dla TF\n",
    "### tymczasową zmienną zewnętrzną z której czerpie dane. Do dalszego działania będziemy potrzebować zmiennych.\n",
    "### Zwróć uwagę gdzie w TensorFlow są wielkie litery jak na przykład w tf.Variable. To Ci oszczędzi stresów.\n",
    "\n",
    "# Tworzymy zmienne gdzie model będzie zwracał Wagi (weights) i Obciążenia (biases). Metoda tf.random działa\n",
    "# w sposób podobny do tego co stworzyliśmy na poprzednim arkuszu. Szczegółowo o niej później.\n",
    "# Kształty zmiennych są rzecz jasne takie same jak w zeszycie 1, bo wynikają z tej samej logiki.\n",
    "weights = tf.Variable(tf.random_uniform([input_size,output_size], minval=-0.1, maxval = 0.1))\n",
    "biases = tf.Variable(tf.random_uniform([output_size], minval=-0.1, maxval = 0.1))\n",
    "\n",
    "# Tworzymy zmienne wyjściowe. Metoda tf.matmul działa na podobnej zasadzie do użytego w zeszycie 1\n",
    "# Iloczynu Skalarnego (metoda np.dot - ang. \"dot product\"), ale jest zgeneralizowana do tensorów.\n",
    "# Tak samo w nawiasie wstawiamy tam to co chcemy przez siebie przemnożyć.\n",
    "outputs = tf.matmul(inputs,weights) + biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deklaracja Funkcji Celu*, oraz metody optymalizacji\n",
    "\n",
    "\n",
    "*in. f. straty [podobnej 'L2' użyliśmy w zeszycie 1] - ang. Objective Function\n",
    "\n",
    "Więcej w temacie Funkcji Celu pod: https://pl.wikipedia.org/wiki/Funkcja_celowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deklarujemy strukturę funkcji straty dla normy L2 tak jak w Zeszycie 1.\n",
    "# W TF jest biblioteka takich funkcji 'tf.losses' z której wybieramy\n",
    "# interesującą nas norma L2 (euklidesowa**) jest tam pod hasłem 'mean_squared_error'.\n",
    "# Atrybutem pierwszym jest 'labels' reprezentujące klucz prawidłowych odpowiedzi dla uczenia się\n",
    "# funkcji. Np \"1\" jeśli na danym \"obrazku\" widnieje \"1\" a funkcja ma kilka odpowiedzi do wyboru.\n",
    "# Drugi atrybut to 'predictions' - logicznie są to dane wyjściowe, czyli wyniki zgadywania naszego modelu.\n",
    "# Kropka po cyfrze 2 jest tam celowo, żeby na pewno otrzymać w wyniku liczbę typu float.\n",
    "mean_loss = tf.losses.mean_squared_error(labels=targets, predictions=outputs) / 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** O normach, w tym Euklidesowej: https://pl.wikipedia.org/wiki/Przestrze%C5%84_unormowana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deklarujemy metodę optymalizacji (treningu) funkcji. Z zasobów metod 'tf.train' wybieramy tę co ostatnio\n",
    "# czyli metodę gradientu prostego** (GradientDescentOptimizer), oraz ustalamy tempo uczenia się modelu (learning_rate)\n",
    "# Do podfunkcji '.minimize' podstawiamy ustaloną wyżej funkcję straty (mean_loss).\n",
    "optimize = tf.train.GradientDescentOptimizer(learning_rate=0.05).minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** O Metodzie gradientu prostego: https://pl.wikipedia.org/wiki/Metoda_gradientu_prostego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/SpookyProgrammer/anaconda3/envs/tensorenv/lib/python3.6/site-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "# Kanonicznie 'sess' jest zmienną, którą nazywa się sesję TF. Tak jak w Klasach używa się self. Dla klarowności.\n",
    "# Obiekty biblioteki tf nie podlegają automatycznemu wykonywaniu wynikającemu z kolejności jak to bywa w Pythonie.\n",
    "# Jest to użyteczne ze względu na specyfikę uczenia maszynowego, o czym przekonasz się wkrótce.\n",
    "# Wywołanie sesji jest jednoznaczne z wykonaniem zadań TF. Raz wywołana, sesja jest aktywna do czasu zamknięcia\n",
    "# Bądź też zamyka się automatycznie, jeśli użyjemy jej w odpowiedniej strukturze (Zeszyt badań 2a)\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
