{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PANEL GŁÓWNY Numpy ### - Zmienne niezbędne dla wykonania zbioru danych przez Numpy biorą się stąd. \n",
    "# Szczegóły słabiej opisanych - zeszyt kursowy 1\n",
    "\n",
    "observations = 1000  # Ustaw ilość obserwacji (Startowa: 1000)\n",
    "vnum = 1             # Liczba zmiennych\n",
    "noic = 10            # Szum w bazie przypadków treningowych.\n",
    "noit = 1             # Szum wygenerowany dla funkcji testującej (docelowej).\n",
    "model = [13,7,-12]   # Parametry modelu losowego dla funkcji a*xs + b*xz + c + szum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Tworzenie i składowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.random.uniform(low = -noic, high = noic, size =(observations,vnum)) # Gen. zakr. los. zm. wej.(patrz zeszyt 1)\n",
    "zs = np.random.uniform(-noic,noic,(observations,vnum)) # J.w.- bez opisów bo zbędne. Poz. dla orientacji.\n",
    "\n",
    "generated_inputs = np.column_stack((xs,zs)) # Składanie z pow. matrycy w formacie observationsx2. (Patrz zeszyt 1)\n",
    "\n",
    "noise = np.random.uniform(-noit,noit,(observations,1)) # Gen. zakr. los. (Patrz zeszyt 1)\n",
    "\n",
    "generated_targets = model[0]*xs + model[1]*zs + model[2] + noise # Funkcja symulująca zadane cele. Do niej dąży model. (Patrz zeszyt 1)\n",
    "\n",
    "np.savez('My_Nums', inputs = generated_inputs, targets = generated_targets) # Tworzy plik My_Nums.npz w tym samym miejscu co ten zeszyt. \n",
    "# Zawiera podane matryce w formie ndarray, co dla obsługi TF jest bardzo wygodne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tutaj zaczyna się TensorFlow\n",
    "Uwaga odnośnie używania TF - obiekty TF nie wykonują się same.\n",
    "Trzeba im wydać konkretne polecenie. \n",
    "\n",
    "Jeśli chcesz na to spojrzeć z innej strony, \n",
    "to zeszyt badawczy 2a zawiera opis i przykłady poszczególnych elementów TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Panel Główny TF ### - tutaj będę wyciągał zmienne dla modelu TF,\n",
    "# żeby było jasne co jest do czego. Normalnie wszystkie zmienne,\n",
    "# którymi miałbym potencjalnie poźniej manipulować zbieram dla porządku 1 miejscu.\n",
    "\n",
    "input_size = 2 # Ilość zmiennych wejściowych - tutaj mamy dwie (x,z)\n",
    "output_size = 1 # Nasza funkcja ma jedno wyjście - y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Rysujemy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tworzy podstawnik do którego będziemy podstawiać dane wejściowe z naszego zbioru danych w pliku npz.\n",
    "# 'float32' to miara precyzji (32bit) - standard dla Pythona to float64, ale tyle na ogół wystarczy.\n",
    "# W nawiasie kwadratowym znajdują się wymiary podstawianych matryc. 'None' jest w pierwszym nie dlatego,\n",
    "# że nasze dane nie mają pierwszego wymiaru (tj. ilość rekordów), ale że nie musimy tego określać.\n",
    "# To przydatne, bo nie trzeba dzięki temu znać dokładnej ilości obserwacji ani pisać odpowiedniego kodu samemu.\n",
    "# Ważne jedynie, żeby podać ilość zmiennych wejściowych (input_size) i wyjściowych (output_size).\n",
    "inputs = tf.placeholder(tf.float32,[None,input_size])\n",
    "# Podstawnik gdzie podamy funkcji dane testowe (cele) - jest w tym samym kształcie co dane wyjściowe,\n",
    "# więc korzystamy ze zmiennej opisującej ilość zmiennych wyjściowych (output_size).\n",
    "targets = tf.placeholder(tf.float32,[None,output_size])\n",
    "\n",
    "### To były podstawniki. Zauważ, że pomiędzy iteracjami nie zachowują one swojej wartości. Są one dla TF\n",
    "### tymczasową zmienną zewnętrzną z której czerpie dane. Do dalszego działania będziemy potrzebować zmiennych.\n",
    "### Zwróć uwagę gdzie w TensorFlow są wielkie litery jak na przykład w tf.Variable. To Ci oszczędzi stresów.\n",
    "\n",
    "# Tworzymy zmienne gdzie model będzie zwracał Wagi (weights) i Obciążenia (biases). Metoda tf.random działa\n",
    "# w sposób podobny do tego co stworzyliśmy na poprzednim arkuszu. Szczegółowo o niej później.\n",
    "# Kształty zmiennych są rzecz jasne takie same jak w zeszycie 1, bo wynikają z tej samej logiki.\n",
    "weights = tf.Variable(tf.random_uniform([input_size,output_size], minval=-0.1, maxval = 0.1))\n",
    "biases = tf.Variable(tf.random_uniform([output_size], minval=-0.1, maxval = 0.1))\n",
    "\n",
    "# Tworzymy zmienne wyjściowe. Metoda tf.matmul działa na podobnej zasadzie do użytego w zeszycie 1\n",
    "# Iloczynu Skalarnego (metoda np.dot - ang. \"dot product\"), ale jest zgeneralizowana do tensorów.\n",
    "# Tak samo w nawiasie wstawiamy tam to co chcemy przez siebie przemnożyć.\n",
    "outputs = tf.matmul(inputs,weights) + biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Deklaracja Funkcji Celu*, oraz metody optymalizacji\n",
    "\n",
    "\n",
    "*in. f. straty [podobnej 'L2' użyliśmy w zeszycie 1] - ang. Objective Function\n",
    "\n",
    "Więcej w temacie Funkcji Celu pod: https://pl.wikipedia.org/wiki/Funkcja_celowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deklarujemy strukturę funkcji straty dla normy L2 tak jak w Zeszycie 1.\n",
    "# W TF jest biblioteka takich funkcji 'tf.losses' z której wybieramy\n",
    "# interesującą nas norma L2 (euklidesowa**) jest tam pod hasłem 'mean_squared_error'.\n",
    "# Atrybutem pierwszym jest 'labels' reprezentujące klucz prawidłowych odpowiedzi dla uczenia się\n",
    "# funkcji. Np \"1\" jeśli na danym \"obrazku\" widnieje \"1\" a funkcja ma kilka odpowiedzi do wyboru.\n",
    "# Drugi atrybut to 'predictions' - logicznie są to dane wyjściowe, czyli wyniki zgadywania naszego modelu.\n",
    "# Kropka po cyfrze 2 jest tam celowo, żeby na pewno otrzymać w wyniku liczbę typu float.\n",
    "mean_loss = tf.losses.mean_squared_error(labels=targets, predictions=outputs) / 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** O normach, w tym Euklidesowej: https://pl.wikipedia.org/wiki/Przestrze%C5%84_unormowana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deklarujemy metodę optymalizacji (treningu) funkcji. Z zasobów metod 'tf.train' wybieramy tę co ostatnio\n",
    "# czyli metodę gradientu prostego** (GradientDescentOptimizer), oraz ustalamy tempo uczenia się modelu (learning_rate)\n",
    "# Do podfunkcji '.minimize' podstawiamy ustaloną wyżej funkcję straty (mean_loss).\n",
    "optimize = tf.train.GradientDescentOptimizer(learning_rate=0.05).minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** O Metodzie gradientu prostego: https://pl.wikipedia.org/wiki/Metoda_gradientu_prostego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Przygotowanie do wykonywania zmiennych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kanonicznie 'sess' jest zmienną, którą nazywa się sesję TF. Tak jak w Klasach używa się self. Dla klarowności.\n",
    "# Obiekty biblioteki tf nie podlegają automatycznemu wykonywaniu wynikającemu z kolejności jak to bywa w Pythonie.\n",
    "# Jest to użyteczne ze względu na specyfikę uczenia maszynowego, o czym przekonasz się wkrótce.\n",
    "# Wywołanie sesji jest jednoznaczne z wykonaniem zadań TF. Raz wywołana, sesja jest aktywna do czasu zamknięcia\n",
    "# Bądź też zamyka się automatycznie, jeśli użyjemy jej w odpowiedniej strukturze (Zeszyt badań 2a)\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Inicjalizacja zmiennych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To jest ta klasyczna część właściwie każdego skryptu TF - Inicjalizator zmiennych\n",
    "# W poprzednich wersjach TF nazywał się inaczej, jednak działa tak samo. (por. Zeszyt badań 2)\n",
    "# W tym momencie przypisujemy go do zmiennej 'initializer' żeby łatwiej później go wywołać w sesji.\n",
    "initializer = tf.global_variables_initializer()\n",
    "\n",
    "# W tym miejscu wywołujemy zdefiniowane wcześniej, zainicjowane zmienne.\n",
    "sess.run(initializer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Wczytywanie danych treningowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przypisujemy wygenerowane wcześniej w NumPy dane w formacie .npz do zmiennej 'training_data'\n",
    "# za pomocą np.load - zauważ, że to co NumPy kodował, teraz rozkodowuje.\n",
    "# Ta metoda ładuje plik z tego samego miejsca w którym znajduje się skrypt,\n",
    "# chyba, że podasz inną ścieżkę.\n",
    "training_data = np.load('My_Nums.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Uczenie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3724.3389\n",
      "1822.4303\n",
      "909.4385\n",
      "467.01273\n",
      "250.24538\n",
      "142.51315\n",
      "87.8728\n",
      "59.30484\n",
      "43.674843\n",
      "34.557796\n",
      "28.789919\n",
      "24.801207\n",
      "21.804653\n",
      "19.39995\n",
      "17.378742\n",
      "15.628785\n",
      "14.086336\n",
      "12.712594\n",
      "11.48182\n",
      "10.375473\n",
      "9.379104\n",
      "8.480856\n",
      "7.670585\n",
      "6.939448\n",
      "6.279587\n",
      "5.6840086\n",
      "5.146416\n",
      "4.6611476\n",
      "4.223101\n",
      "3.8276823\n",
      "3.470737\n",
      "3.14852\n",
      "2.857651\n",
      "2.5950842\n",
      "2.3580673\n",
      "2.1441102\n",
      "1.950967\n",
      "1.7766192\n",
      "1.6192317\n",
      "1.4771603\n",
      "1.348909\n",
      "1.2331411\n",
      "1.1286324\n",
      "1.0342915\n",
      "0.94913244\n",
      "0.87225735\n",
      "0.8028611\n",
      "0.74021804\n",
      "0.6836697\n",
      "0.63262355\n",
      "0.5865435\n",
      "0.54494524\n",
      "0.5073963\n",
      "0.47350007\n",
      "0.44290167\n",
      "0.4152799\n",
      "0.3903475\n",
      "0.36783895\n",
      "0.34752038\n",
      "0.32918036\n",
      "0.3126244\n",
      "0.2976779\n",
      "0.28418675\n",
      "0.27200654\n",
      "0.26101282\n",
      "0.25108922\n",
      "0.24212964\n",
      "0.23404309\n",
      "0.22674277\n",
      "0.22015317\n",
      "0.2142039\n",
      "0.20883414\n",
      "0.20398614\n",
      "0.1996106\n",
      "0.19565989\n",
      "0.19209424\n",
      "0.18887536\n",
      "0.18596958\n",
      "0.18334681\n",
      "0.18097922\n",
      "0.17884193\n",
      "0.17691232\n",
      "0.17517087\n",
      "0.17359862\n",
      "0.17217901\n",
      "0.17089795\n",
      "0.16974144\n",
      "0.16869746\n",
      "0.16775501\n",
      "0.1669042\n",
      "0.16613598\n",
      "0.16544299\n",
      "0.16481702\n",
      "0.16425243\n",
      "0.16374236\n",
      "0.16328199\n",
      "0.16286658\n",
      "0.1624913\n",
      "0.16215278\n",
      "0.16184726\n"
     ]
    }
   ],
   "source": [
    "# Ustalamy ilość iteracji. Kanonicznie jest to 'e', dla 'epoki' (ang. 'epoch')\n",
    "# Dla maszyny każda kolejna iteracja w której się uczy to właśnie 'epoka'.\n",
    "for e in range(100):\n",
    "    # Pętla zwraca wartość dla operacji 'optimize', oraz 'mean_loss' - Patrz. pkt.4 \"Deklaracja Funkcji Celu\",\n",
    "    # dla każdej 'epoki' tzn. 'None' dla 'optimize', do czego stosujemy '_', żeby zignorować tę wartość,\n",
    "    # oraz 'curr_loss' (ang. 'current loss' - aktualna strata) dla funkcji straty 'mean_loss'.\n",
    "    _, curr_loss = sess.run([optimize, mean_loss],\n",
    "                            # Logika sess.run jest taka, że w pierwszym parametrze podstawiamy listę [w, nawiasach_kwadratowych]\n",
    "                            # tych rzeczy, które chcemy, aby zostały wykonane,\n",
    "                           feed_dict = {inputs: training_data['inputs'], targets: training_data['targets']})\n",
    "                            # zaś drugi parametr, feed_dict to słownik z którego czerpie algorytm.\n",
    "                            # Składnia: feed_dict = {podstawnik1 : dane, podstawnik2 : dane},\n",
    "                            # Logika działa tu w naszym wypadku następująco: \"Weż dane opisane jako 'inputs' ze zmiennej training_data\n",
    "                            # i przypisz je do podstawnika 'inputs'. Potem weź dane opisane jako 'targets' ze zmiennej training_data\n",
    "                            # i przypisz je do podstawnika 'targets'. Zmienna training_data to jak pamiętamy nasze dane NumPy (pkt. 7).\n",
    "  \n",
    "    print(curr_loss) #Drukuje listę funkcji celu dla wszystkich epok po kolei, dzięki czemu widzimy jak model się \"uczy\".    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
