{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wzór TensorFlow dla głębokiej sieci neuronowej\n",
    "\n",
    "W tym przykładzie opisuję części deep neural network na przykładzie klaycznej numerycznej bazy MNIST.\n",
    "MNIST to baza ręcznie pisanych cyfr w zakresie 0-9 z którą praktycznie każdy w branży miał kiedyś styczność.\n",
    "Planuję zastosować ten wzornik do swoich celów później, gdyż otrzymałem podobne zadanie testowe.\n",
    "Miej na względzie, że ten wzór nie pokazuje w jaki sposób preprocesować przygotować, preprocesować, ani dzielić bazę danych na zbiory (treningowy/walidacyjny/testowy), a jedynie pokazuje jak zbudować algorytm sieci deep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatywne źródło danych MNIST\n",
    "Jak ostatnio sprawdzałem, to dane ze strony Yann wciąż działają."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obejście przestarzałego kodu bazy MNIST\n",
    "Jak spróbowałem użyć prostem wersji z tutoriala, to dostałem info, że jest przestarzała i zaraz wycofują.\n",
    "Poniższe obejście powinno załatwić sprawę. Kiedyś wystarczała do tego 1 linijka kodu.\n",
    "Teraz podziwiajcie co się porobiło... Znalezione na oficjalnej stronie twórców TF, gdzie prowadził kod błędu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poniższy kod to de-facto official/mnist.py, o którym mowa jak się próbuje\n",
    "# używać przestarzałej funkcji read_data_sets. Ogólnie używa się tego kodu,\n",
    "# który swego czasu już próbowałem rozpracować, a który używał znajomy hindus\n",
    "#, którego kod analizuję w folderze \"spyder\" niniejszego repozytorium.\n",
    "# Są do skrypty \"board.py i importowane do niego \"input_data\".\n",
    "# Postaram się to tutaj przeanalizować jeszcze raz i bardziej szczegółowo, bo widać\n",
    "# trudno będzie tego kawałka uniknąć.\n",
    "# Jeszcze zobaczę na ile jest to absolutnie konieczne, a na ile można to zostawić jako\n",
    "# ciekawostkę w stylu \"obchodzimy przestarzały MNIST\". Na razie skupię się na kodzie głównym\n",
    "# tak, aby reszta działała bez większych problemów. Wszelkie modyfikacje, których tu dokonuję\n",
    "# powodowane były wyskakującymi na bieżąco błędami.\n",
    "\n",
    "#  Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#  you may not use this file except in compliance with the License.\n",
    "#  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "#  limitations under the License.\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gzip\n",
    "\n",
    "import numpy as np\n",
    "import numpy\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import random_seed\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "import collections\n",
    "import csv\n",
    "import os\n",
    "from os import path\n",
    "import random\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "from six.moves import urllib\n",
    "\n",
    "from tensorflow.contrib.framework import deprecated\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "# W tym miejscu definiujemy inne źródło danych MNIST. Strona Yann w komentarzu.\n",
    "# OStatnio jak sprawdzałem działała.\n",
    "# CVDF mirror of http://yann.lecun.com/exdb/mnist/\n",
    "DEFAULT_SOURCE_URL = 'https://storage.googleapis.com/cvdf-datasets/mnist/'\n",
    "\n",
    "Dataset = collections.namedtuple('Dataset', ['data', 'target'])\n",
    "Datasets = collections.namedtuple('Datasets', ['train', 'validation', 'test'])\n",
    "\n",
    "\n",
    "def load_csv_with_header(filename,\n",
    "                         target_dtype,\n",
    "                         features_dtype,\n",
    "                         target_column=-1):\n",
    "  \"\"\"Load dataset from CSV file with a header row.\"\"\"\n",
    "  with gfile.Open(filename) as csv_file:\n",
    "    data_file = csv.reader(csv_file)\n",
    "    header = next(data_file)\n",
    "    n_samples = int(header[0])\n",
    "    n_features = int(header[1])\n",
    "    data = np.zeros((n_samples, n_features), dtype=features_dtype)\n",
    "    target = np.zeros((n_samples,), dtype=target_dtype)\n",
    "    for i, row in enumerate(data_file):\n",
    "      target[i] = np.asarray(row.pop(target_column), dtype=target_dtype)\n",
    "      data[i] = np.asarray(row, dtype=features_dtype)\n",
    "\n",
    "  return Dataset(data=data, target=target)\n",
    "\n",
    "\n",
    "def load_csv_without_header(filename,\n",
    "                            target_dtype,\n",
    "                            features_dtype,\n",
    "                            target_column=-1):\n",
    "  \"\"\"Load dataset from CSV file without a header row.\"\"\"\n",
    "  with gfile.Open(filename) as csv_file:\n",
    "    data_file = csv.reader(csv_file)\n",
    "    data, target = [], []\n",
    "    for row in data_file:\n",
    "      target.append(row.pop(target_column))\n",
    "      data.append(np.asarray(row, dtype=features_dtype))\n",
    "\n",
    "  target = np.array(target, dtype=target_dtype)\n",
    "  data = np.array(data)\n",
    "  return Dataset(data=data, target=target)\n",
    "\n",
    "\n",
    "def shrink_csv(filename, ratio):\n",
    "  \"\"\"Create a smaller dataset of only 1/ratio of original data.\"\"\"\n",
    "  filename_small = filename.replace('.', '_small.')\n",
    "  with gfile.Open(filename_small, 'w') as csv_file_small:\n",
    "    writer = csv.writer(csv_file_small)\n",
    "    with gfile.Open(filename) as csv_file:\n",
    "      reader = csv.reader(csv_file)\n",
    "      i = 0\n",
    "      for row in reader:\n",
    "        if i % ratio == 0:\n",
    "          writer.writerow(row)\n",
    "        i += 1\n",
    "\n",
    "\n",
    "def load_iris(data_path=None):\n",
    "  \"\"\"Load Iris dataset.\n",
    "  Args:\n",
    "      data_path: string, path to iris dataset (optional)\n",
    "  Returns:\n",
    "    Dataset object containing data in-memory.\n",
    "  \"\"\"\n",
    "  if data_path is None:\n",
    "    module_path = path.dirname(__file__)\n",
    "    data_path = path.join(module_path, 'data', 'iris.csv')\n",
    "  return load_csv_with_header(\n",
    "      data_path,\n",
    "      target_dtype=np.int,\n",
    "      features_dtype=np.float)\n",
    "\n",
    "\n",
    "def load_boston(data_path=None):\n",
    "  \"\"\"Load Boston housing dataset.\n",
    "  Args:\n",
    "      data_path: string, path to boston dataset (optional)\n",
    "  Returns:\n",
    "    Dataset object containing data in-memory.\n",
    "  \"\"\"\n",
    "  if data_path is None:\n",
    "    module_path = path.dirname(__file__)\n",
    "    data_path = path.join(module_path, 'data', 'boston_house_prices.csv')\n",
    "  return load_csv_with_header(\n",
    "      data_path,\n",
    "      target_dtype=np.float,\n",
    "      features_dtype=np.float)\n",
    "\n",
    "\n",
    "def retry(initial_delay,\n",
    "          max_delay,\n",
    "          factor=2.0,\n",
    "          jitter=0.25,\n",
    "          is_retriable=None):\n",
    "  \"\"\"Simple decorator for wrapping retriable functions.\n",
    "  Args:\n",
    "    initial_delay: the initial delay.\n",
    "    factor: each subsequent retry, the delay is multiplied by this value.\n",
    "        (must be >= 1).\n",
    "    jitter: to avoid lockstep, the returned delay is multiplied by a random\n",
    "        number between (1-jitter) and (1+jitter). To add a 20% jitter, set\n",
    "        jitter = 0.2. Must be < 1.\n",
    "    max_delay: the maximum delay allowed (actual max is\n",
    "        max_delay * (1 + jitter).\n",
    "    is_retriable: (optional) a function that takes an Exception as an argument\n",
    "        and returns true if retry should be applied.\n",
    "  \"\"\"\n",
    "  if factor < 1:\n",
    "    raise ValueError('factor must be >= 1; was %f' % (factor,))\n",
    "\n",
    "  if jitter >= 1:\n",
    "    raise ValueError('jitter must be < 1; was %f' % (jitter,))\n",
    "\n",
    "  # Generator to compute the individual delays\n",
    "  def delays():\n",
    "    delay = initial_delay\n",
    "    while delay <= max_delay:\n",
    "      yield delay * random.uniform(1 - jitter,  1 + jitter)\n",
    "      delay *= factor\n",
    "\n",
    "  def wrap(fn):\n",
    "    \"\"\"Wrapper function factory invoked by decorator magic.\"\"\"\n",
    "\n",
    "    def wrapped_fn(*args, **kwargs):\n",
    "      \"\"\"The actual wrapper function that applies the retry logic.\"\"\"\n",
    "      for delay in delays():\n",
    "        try:\n",
    "          return fn(*args, **kwargs)\n",
    "        except Exception as e:  # pylint: disable=broad-except)\n",
    "          if is_retriable is None:\n",
    "            continue\n",
    "\n",
    "          if is_retriable(e):\n",
    "            time.sleep(delay)\n",
    "          else:\n",
    "            raise\n",
    "      return fn(*args, **kwargs)\n",
    "    return wrapped_fn\n",
    "  return wrap\n",
    "\n",
    "\n",
    "_RETRIABLE_ERRNOS = {\n",
    "    110,  # Connection timed out [socket.py]\n",
    "}\n",
    "\n",
    "\n",
    "def _is_retriable(e):\n",
    "  return isinstance(e, IOError) and e.errno in _RETRIABLE_ERRNOS\n",
    "\n",
    "\n",
    "@retry(initial_delay=1.0, max_delay=16.0, is_retriable=_is_retriable)\n",
    "def urlretrieve_with_retry(url, filename=None):\n",
    "  return urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "\n",
    "def maybe_download(filename, work_directory, source_url):\n",
    "  \"\"\"Download the data from source url, unless it's already here.\n",
    "  Args:\n",
    "      filename: string, name of the file in the directory.\n",
    "      work_directory: string, path to working directory.\n",
    "      source_url: url to download from if file doesn't exist.\n",
    "  Returns:\n",
    "      Path to resulting file.\n",
    "  \"\"\"\n",
    "  if not gfile.Exists(work_directory):\n",
    "    gfile.MakeDirs(work_directory)\n",
    "  filepath = os.path.join(work_directory, filename)\n",
    "  if not gfile.Exists(filepath):\n",
    "    temp_file_name, _ = urlretrieve_with_retry(source_url)\n",
    "    gfile.Copy(temp_file_name, filepath)\n",
    "    with gfile.GFile(filepath) as f:\n",
    "      size = f.size()\n",
    "    print('Successfully downloaded', filename, size, 'bytes.')\n",
    "  return filepath\n",
    "\n",
    "def _read32(bytestream):\n",
    "  dt = numpy.dtype(numpy.uint32).newbyteorder('>')\n",
    "  return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]\n",
    "\n",
    "\n",
    "def extract_images(f):\n",
    "  \"\"\"Extract the images into a 4D uint8 numpy array [index, y, x, depth].\n",
    "  Args:\n",
    "    f: A file object that can be passed into a gzip reader.\n",
    "  Returns:\n",
    "    data: A 4D uint8 numpy array [index, y, x, depth].\n",
    "  Raises:\n",
    "    ValueError: If the bytestream does not start with 2051.\n",
    "  \"\"\"\n",
    "  print('Extracting', f.name)\n",
    "  with gzip.GzipFile(fileobj=f) as bytestream:\n",
    "    magic = _read32(bytestream)\n",
    "    if magic != 2051:\n",
    "      raise ValueError('Invalid magic number %d in MNIST image file: %s' %\n",
    "                       (magic, f.name))\n",
    "    num_images = _read32(bytestream)\n",
    "    rows = _read32(bytestream)\n",
    "    cols = _read32(bytestream)\n",
    "    buf = bytestream.read(rows * cols * num_images)\n",
    "    data = numpy.frombuffer(buf, dtype=numpy.uint8)\n",
    "    data = data.reshape(num_images, rows, cols, 1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "  \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "  num_labels = labels_dense.shape[0]\n",
    "  index_offset = numpy.arange(num_labels) * num_classes\n",
    "  labels_one_hot = numpy.zeros((num_labels, num_classes))\n",
    "  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "  return labels_one_hot\n",
    "\n",
    "\n",
    "def extract_labels(f, one_hot=False, num_classes=10):\n",
    "  \"\"\"Extract the labels into a 1D uint8 numpy array [index].\n",
    "  Args:\n",
    "    f: A file object that can be passed into a gzip reader.\n",
    "    one_hot: Does one hot encoding for the result.\n",
    "    num_classes: Number of classes for the one hot encoding.\n",
    "  Returns:\n",
    "    labels: a 1D uint8 numpy array.\n",
    "  Raises:\n",
    "    ValueError: If the bystream doesn't start with 2049.\n",
    "  \"\"\"\n",
    "  print('Extracting', f.name)\n",
    "  with gzip.GzipFile(fileobj=f) as bytestream:\n",
    "    magic = _read32(bytestream)\n",
    "    if magic != 2049:\n",
    "      raise ValueError('Invalid magic number %d in MNIST label file: %s' %\n",
    "                       (magic, f.name))\n",
    "    num_items = _read32(bytestream)\n",
    "    buf = bytestream.read(num_items)\n",
    "    labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n",
    "    if one_hot:\n",
    "      return dense_to_one_hot(labels, num_classes)\n",
    "    return labels\n",
    "\n",
    "\n",
    "class DataSet(object):\n",
    "\n",
    "  def __init__(self,\n",
    "               images,\n",
    "               labels,\n",
    "               fake_data=False,\n",
    "               one_hot=False,\n",
    "               dtype=dtypes.float32,\n",
    "               reshape=True,\n",
    "               seed=None):\n",
    "    \"\"\"Construct a DataSet.\n",
    "    one_hot arg is used only if fake_data is true.  `dtype` can be either\n",
    "    `uint8` to leave the input as `[0, 255]`, or `float32` to rescale into\n",
    "    `[0, 1]`.  Seed arg provides for convenient deterministic testing.\n",
    "    \"\"\"\n",
    "    seed1, seed2 = random_seed.get_seed(seed)\n",
    "    # If op level seed is not set, use whatever graph level seed is returned\n",
    "    numpy.random.seed(seed1 if seed is None else seed2)\n",
    "    dtype = dtypes.as_dtype(dtype).base_dtype\n",
    "    if dtype not in (dtypes.uint8, dtypes.float32):\n",
    "      raise TypeError(\n",
    "          'Invalid image dtype %r, expected uint8 or float32' % dtype)\n",
    "    if fake_data:\n",
    "      self._num_examples = 10000\n",
    "      self.one_hot = one_hot\n",
    "    else:\n",
    "      assert images.shape[0] == labels.shape[0], (\n",
    "          'images.shape: %s labels.shape: %s' % (images.shape, labels.shape))\n",
    "      self._num_examples = images.shape[0]\n",
    "\n",
    "      # Convert shape from [num examples, rows, columns, depth]\n",
    "      # to [num examples, rows*columns] (assuming depth == 1)\n",
    "      if reshape:\n",
    "        assert images.shape[3] == 1\n",
    "        images = images.reshape(images.shape[0],\n",
    "                                images.shape[1] * images.shape[2])\n",
    "      if dtype == dtypes.float32:\n",
    "        # Convert from [0, 255] -> [0.0, 1.0].\n",
    "        images = images.astype(numpy.float32)\n",
    "        images = numpy.multiply(images, 1.0 / 255.0)\n",
    "    self._images = images\n",
    "    self._labels = labels\n",
    "    self._epochs_completed = 0\n",
    "    self._index_in_epoch = 0\n",
    "\n",
    "  @property\n",
    "  def images(self):\n",
    "    return self._images\n",
    "\n",
    "  @property\n",
    "  def labels(self):\n",
    "    return self._labels\n",
    "\n",
    "  @property\n",
    "  def num_examples(self):\n",
    "    return self._num_examples\n",
    "\n",
    "  @property\n",
    "  def epochs_completed(self):\n",
    "    return self._epochs_completed\n",
    "\n",
    "  def next_batch(self, batch_size, fake_data=False, shuffle=True):\n",
    "    \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "    if fake_data:\n",
    "      fake_image = [1] * 784\n",
    "      if self.one_hot:\n",
    "        fake_label = [1] + [0] * 9\n",
    "      else:\n",
    "        fake_label = 0\n",
    "      return [fake_image for _ in xrange(batch_size)], [\n",
    "          fake_label for _ in xrange(batch_size)\n",
    "      ]\n",
    "    start = self._index_in_epoch\n",
    "    # Shuffle for the first epoch\n",
    "    if self._epochs_completed == 0 and start == 0 and shuffle:\n",
    "      perm0 = numpy.arange(self._num_examples)\n",
    "      numpy.random.shuffle(perm0)\n",
    "      self._images = self.images[perm0]\n",
    "      self._labels = self.labels[perm0]\n",
    "    # Go to the next epoch\n",
    "    if start + batch_size > self._num_examples:\n",
    "      # Finished epoch\n",
    "      self._epochs_completed += 1\n",
    "      # Get the rest examples in this epoch\n",
    "      rest_num_examples = self._num_examples - start\n",
    "      images_rest_part = self._images[start:self._num_examples]\n",
    "      labels_rest_part = self._labels[start:self._num_examples]\n",
    "      # Shuffle the data\n",
    "      if shuffle:\n",
    "        perm = numpy.arange(self._num_examples)\n",
    "        numpy.random.shuffle(perm)\n",
    "        self._images = self.images[perm]\n",
    "        self._labels = self.labels[perm]\n",
    "      # Start next epoch\n",
    "      start = 0\n",
    "      self._index_in_epoch = batch_size - rest_num_examples\n",
    "      end = self._index_in_epoch\n",
    "      images_new_part = self._images[start:end]\n",
    "      labels_new_part = self._labels[start:end]\n",
    "      return numpy.concatenate(\n",
    "          (images_rest_part, images_new_part), axis=0), numpy.concatenate(\n",
    "              (labels_rest_part, labels_new_part), axis=0)\n",
    "    else:\n",
    "      self._index_in_epoch += batch_size\n",
    "      end = self._index_in_epoch\n",
    "      return self._images[start:end], self._labels[start:end]\n",
    "\n",
    "\n",
    "def read_data_sets(train_dir,\n",
    "                   fake_data=False,\n",
    "                   one_hot=False,\n",
    "                   dtype=dtypes.float32,\n",
    "                   reshape=True,\n",
    "                   validation_size=5000,\n",
    "                   seed=None,\n",
    "                   source_url=DEFAULT_SOURCE_URL):\n",
    "  if fake_data:\n",
    "\n",
    "    def fake():\n",
    "      return DataSet(\n",
    "          [], [], fake_data=True, one_hot=one_hot, dtype=dtype, seed=seed)\n",
    "\n",
    "    train = fake()\n",
    "    validation = fake()\n",
    "    test = fake()\n",
    "    return Datasets(train=train, validation=validation, test=test)\n",
    "\n",
    "  if not source_url:  # empty string check\n",
    "    source_url = DEFAULT_SOURCE_URL\n",
    "\n",
    "  TRAIN_IMAGES = 'train-images-idx3-ubyte.gz'\n",
    "  TRAIN_LABELS = 'train-labels-idx1-ubyte.gz'\n",
    "  TEST_IMAGES = 't10k-images-idx3-ubyte.gz'\n",
    "  TEST_LABELS = 't10k-labels-idx1-ubyte.gz'\n",
    "\n",
    "  local_file = maybe_download(TRAIN_IMAGES, train_dir,\n",
    "                                   source_url + TRAIN_IMAGES)\n",
    "  with gfile.Open(local_file, 'rb') as f:\n",
    "    train_images = extract_images(f)\n",
    "\n",
    "  local_file = maybe_download(TRAIN_LABELS, train_dir,\n",
    "                                   source_url + TRAIN_LABELS)\n",
    "  with gfile.Open(local_file, 'rb') as f:\n",
    "    train_labels = extract_labels(f, one_hot=one_hot)\n",
    "\n",
    "  local_file = maybe_download(TEST_IMAGES, train_dir,\n",
    "                                   source_url + TEST_IMAGES)\n",
    "  with gfile.Open(local_file, 'rb') as f:\n",
    "    test_images = extract_images(f)\n",
    "\n",
    "  local_file = maybe_download(TEST_LABELS, train_dir,\n",
    "                                   source_url + TEST_LABELS)\n",
    "  with gfile.Open(local_file, 'rb') as f:\n",
    "    test_labels = extract_labels(f, one_hot=one_hot)\n",
    "\n",
    "  if not 0 <= validation_size <= len(train_images):\n",
    "    raise ValueError('Validation size should be between 0 and {}. Received: {}.'\n",
    "                     .format(len(train_images), validation_size))\n",
    "\n",
    "  validation_images = train_images[:validation_size]\n",
    "  validation_labels = train_labels[:validation_size]\n",
    "  train_images = train_images[validation_size:]\n",
    "  train_labels = train_labels[validation_size:]\n",
    "\n",
    "  options = dict(dtype=dtype, reshape=reshape, seed=seed)\n",
    "\n",
    "  train = DataSet(train_images, train_labels, **options)\n",
    "  validation = DataSet(validation_images, validation_labels, **options)\n",
    "  test = DataSet(test_images, test_labels, **options)\n",
    "\n",
    "  return Datasets(train=train, validation=validation, test=test)\n",
    "\n",
    "\n",
    "def load_mnist(train_dir='MNIST-data'):\n",
    "  return read_data_sets(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Ściągamy biblioteki operacyjne. Jak nie wiesz co to jest cofnij się do poprzednich zeszytów kursowych.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Poniższy kod ściąga bazę MNIST i przyporządkowuje ją do zmiennej.\n",
    "# Jest to duże ułatwienie na poziomie szkolenia względem tego co robiono kiedyś, \n",
    "# a co możesz sobie obejrzeć w kodzie działającym właściwie identycznie w folderze \"spyder\"\n",
    "# od którego rozpocząłem swoją analizę. Z czasem społeczność stworzyła szereg ułatwień\n",
    "# i paczka startowa samouczka jest przewidziana jako opcjonalny download ze strony twórcy.\n",
    "# Dane są już preprocesowane, oraz rozbite na treningowe, walidacyjne i testowe, \n",
    "# zatem i o to nie musisz się tym razem martwić. \n",
    "# PRZESTARZAŁY KOD: from tensorflow.examples.tutorials.mnist import input_data\n",
    "# UWAGA: Do powyższego kodu zastosowano obejście stworzone przez twórców TF. (Ta wielka kobyła powyżej)\n",
    "# Używanie go pomimo ostrzeżeń o wycofywaniu grozi błędem i w ogóle... :P\n",
    "\n",
    "# Read_data_sets ma dwa argumenty. Pierwszy mówi funkcji o lokalizacji danych a drugi o sposobie\n",
    "# przyporządkowywania (kodowania pozycji) zmiennych. One_hot jest dosyć popularne, gdyż sprawdza się przy\n",
    "# niewielkiej liczbie argumentów i nie generuje niepotrzebnych korelacji między zmiennymi.\n",
    "# Jeśli jednak masz dużo zmiennych, to warto zastosować binary (czyli one_hot=\"False\"), gdyż posiadanie\n",
    "# np. kilkuset, lub kilkunastu tysięcy zmiennych kodujących pozycję będzie dość zasobożerne...\n",
    "mnist = read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
