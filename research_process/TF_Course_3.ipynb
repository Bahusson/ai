{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wzór TensorFlow dla głębokiej sieci neuronowej\n",
    "\n",
    "W tym przykładzie opisuję części deep neural network na przykładzie klasycznej numerycznej bazy MNIST.\n",
    "MNIST to baza ręcznie pisanych cyfr w zakresie 0-9 z którą praktycznie każdy w branży miał kiedyś styczność.\n",
    "Planuję zastosować ten wzornik do swoich celów później, gdyż otrzymałem podobne zadanie testowe.\n",
    "Miej na względzie, że ten wzór nie pokazuje w jaki sposób preprocesować przygotować, preprocesować, ani dzielić bazę danych na zbiory (treningowy/walidacyjny/testowy), a jedynie pokazuje jak zbudować algorytm sieci deep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Obejście przestarzałego kodu bazy MNIST\n",
    "Jak spróbowałem użyć prostem wersji z tutoriala, to dostałem info, że jest przestarzała i zaraz wycofują.\n",
    "Poniższe obejście powinno załatwić sprawę. Kiedyś wystarczała do tego 1 linijka kodu.\n",
    "Teraz podziwiajcie co się porobiło... Znalezione na oficjalnej stronie twórców TF, gdzie prowadził kod błędu.\n",
    "\n",
    "Poniższy kod to de-facto official/mnist.py, o którym mowa jak się próbuje używać przestarzałej funkcji read_data_sets. Ogólnie używa się tego kodu, który swego czasu już próbowałem rozpracować, a który używał znajomy hindus, którego kod analizuję w folderze \"spyder\" niniejszego repozytorium. Są do skrypty \"board.py i importowane do niego \"input_data\". Postaram się to tutaj przeanalizować jeszcze raz i bardziej szczegółowo, bo widać\n",
    "trudno będzie tego kawałka uniknąć. \n",
    "\n",
    "Pamiętaj, że w takich sytuacjach możesz zawsze zrobić to co zrobił kolega z Indii i po prostu podzielić kod na dwie części, a następnie zaimportować sobie jedną na początku kodu. Na pewno uporządkuje Ci do miejsce pracy, co jest cenne zwłaszcza w Jupyterze, gdzie zeszyty bez importowania mają tentencję do ciągnięcia się kilometrami.\n",
    "\n",
    "Jeszcze zobaczę na ile jest to absolutnie konieczne, a na ile można to zostawić jako ciekawostkę w stylu \"obchodzimy przestarzały MNIST\". Na razie skupię się na kodzie głównym tak, aby reszta działała bez większych problemów.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uwaga: Alternatywne źródło danych MNIST\n",
    "Jak ostatnio sprawdzałem, to dane ze strony Yann wciąż działają, ale w linijce 57 wrzuciłem alternatywne źródło. \n",
    "\n",
    "Nie rozbijam kodu i nie wrzucam tego komentarza celowo, żeby nie robić bałaganu, ale chcę, żebyś zwrócił(a) na to uwagę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W komentarzu poniżej oryginalna licencja. \n",
    "# Jest to standardowa licencja Apache 2.0., która dodatkowo mówi nam ni mniej ni więcej tyle, \n",
    "# że twórcy TF nie odpowiadają za użytkowanie przez nas tego obejścia.\n",
    "# Wszelkie zmiany do tego kodu wymuszone były na mnie tym, że inaczej nie działał. ;)\n",
    "# Komentarze w języku polskim dla lepszego zrozumienia działania mechanizmu, jeśli się za to wezmę.\n",
    "\n",
    "#  Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#  you may not use this file except in compliance with the License.\n",
    "#  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "#  limitations under the License.\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gzip\n",
    "\n",
    "import numpy as np\n",
    "import numpy\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework import random_seed\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "import collections\n",
    "import csv\n",
    "import os\n",
    "from os import path\n",
    "import random\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "from six.moves import urllib\n",
    "\n",
    "from tensorflow.contrib.framework import deprecated\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "# W tym miejscu definiujemy inne źródło danych MNIST. Strona Yann w komentarzu.\n",
    "# OStatnio jak sprawdzałem działała.\n",
    "# CVDF mirror of http://yann.lecun.com/exdb/mnist/\n",
    "DEFAULT_SOURCE_URL = 'https://storage.googleapis.com/cvdf-datasets/mnist/'\n",
    "\n",
    "Dataset = collections.namedtuple('Dataset', ['data', 'target'])\n",
    "Datasets = collections.namedtuple('Datasets', ['train', 'validation', 'test'])\n",
    "\n",
    "\n",
    "def load_csv_with_header(filename,\n",
    "                         target_dtype,\n",
    "                         features_dtype,\n",
    "                         target_column=-1):\n",
    "  \"\"\"Load dataset from CSV file with a header row.\"\"\"\n",
    "  with gfile.Open(filename) as csv_file:\n",
    "    data_file = csv.reader(csv_file)\n",
    "    header = next(data_file)\n",
    "    n_samples = int(header[0])\n",
    "    n_features = int(header[1])\n",
    "    data = np.zeros((n_samples, n_features), dtype=features_dtype)\n",
    "    target = np.zeros((n_samples,), dtype=target_dtype)\n",
    "    for i, row in enumerate(data_file):\n",
    "      target[i] = np.asarray(row.pop(target_column), dtype=target_dtype)\n",
    "      data[i] = np.asarray(row, dtype=features_dtype)\n",
    "\n",
    "  return Dataset(data=data, target=target)\n",
    "\n",
    "\n",
    "def load_csv_without_header(filename,\n",
    "                            target_dtype,\n",
    "                            features_dtype,\n",
    "                            target_column=-1):\n",
    "  \"\"\"Load dataset from CSV file without a header row.\"\"\"\n",
    "  with gfile.Open(filename) as csv_file:\n",
    "    data_file = csv.reader(csv_file)\n",
    "    data, target = [], []\n",
    "    for row in data_file:\n",
    "      target.append(row.pop(target_column))\n",
    "      data.append(np.asarray(row, dtype=features_dtype))\n",
    "\n",
    "  target = np.array(target, dtype=target_dtype)\n",
    "  data = np.array(data)\n",
    "  return Dataset(data=data, target=target)\n",
    "\n",
    "\n",
    "def shrink_csv(filename, ratio):\n",
    "  \"\"\"Create a smaller dataset of only 1/ratio of original data.\"\"\"\n",
    "  filename_small = filename.replace('.', '_small.')\n",
    "  with gfile.Open(filename_small, 'w') as csv_file_small:\n",
    "    writer = csv.writer(csv_file_small)\n",
    "    with gfile.Open(filename) as csv_file:\n",
    "      reader = csv.reader(csv_file)\n",
    "      i = 0\n",
    "      for row in reader:\n",
    "        if i % ratio == 0:\n",
    "          writer.writerow(row)\n",
    "        i += 1\n",
    "\n",
    "\n",
    "def load_iris(data_path=None):\n",
    "  \"\"\"Load Iris dataset.\n",
    "  Args:\n",
    "      data_path: string, path to iris dataset (optional)\n",
    "  Returns:\n",
    "    Dataset object containing data in-memory.\n",
    "  \"\"\"\n",
    "  if data_path is None:\n",
    "    module_path = path.dirname(__file__)\n",
    "    data_path = path.join(module_path, 'data', 'iris.csv')\n",
    "  return load_csv_with_header(\n",
    "      data_path,\n",
    "      target_dtype=np.int,\n",
    "      features_dtype=np.float)\n",
    "\n",
    "\n",
    "def load_boston(data_path=None):\n",
    "  \"\"\"Load Boston housing dataset.\n",
    "  Args:\n",
    "      data_path: string, path to boston dataset (optional)\n",
    "  Returns:\n",
    "    Dataset object containing data in-memory.\n",
    "  \"\"\"\n",
    "  if data_path is None:\n",
    "    module_path = path.dirname(__file__)\n",
    "    data_path = path.join(module_path, 'data', 'boston_house_prices.csv')\n",
    "  return load_csv_with_header(\n",
    "      data_path,\n",
    "      target_dtype=np.float,\n",
    "      features_dtype=np.float)\n",
    "\n",
    "\n",
    "def retry(initial_delay,\n",
    "          max_delay,\n",
    "          factor=2.0,\n",
    "          jitter=0.25,\n",
    "          is_retriable=None):\n",
    "  \"\"\"Simple decorator for wrapping retriable functions.\n",
    "  Args:\n",
    "    initial_delay: the initial delay.\n",
    "    factor: each subsequent retry, the delay is multiplied by this value.\n",
    "        (must be >= 1).\n",
    "    jitter: to avoid lockstep, the returned delay is multiplied by a random\n",
    "        number between (1-jitter) and (1+jitter). To add a 20% jitter, set\n",
    "        jitter = 0.2. Must be < 1.\n",
    "    max_delay: the maximum delay allowed (actual max is\n",
    "        max_delay * (1 + jitter).\n",
    "    is_retriable: (optional) a function that takes an Exception as an argument\n",
    "        and returns true if retry should be applied.\n",
    "  \"\"\"\n",
    "  if factor < 1:\n",
    "    raise ValueError('factor must be >= 1; was %f' % (factor,))\n",
    "\n",
    "  if jitter >= 1:\n",
    "    raise ValueError('jitter must be < 1; was %f' % (jitter,))\n",
    "\n",
    "  # Generator to compute the individual delays\n",
    "  def delays():\n",
    "    delay = initial_delay\n",
    "    while delay <= max_delay:\n",
    "      yield delay * random.uniform(1 - jitter,  1 + jitter)\n",
    "      delay *= factor\n",
    "\n",
    "  def wrap(fn):\n",
    "    \"\"\"Wrapper function factory invoked by decorator magic.\"\"\"\n",
    "\n",
    "    def wrapped_fn(*args, **kwargs):\n",
    "      \"\"\"The actual wrapper function that applies the retry logic.\"\"\"\n",
    "      for delay in delays():\n",
    "        try:\n",
    "          return fn(*args, **kwargs)\n",
    "        except Exception as e:  # pylint: disable=broad-except)\n",
    "          if is_retriable is None:\n",
    "            continue\n",
    "\n",
    "          if is_retriable(e):\n",
    "            time.sleep(delay)\n",
    "          else:\n",
    "            raise\n",
    "      return fn(*args, **kwargs)\n",
    "    return wrapped_fn\n",
    "  return wrap\n",
    "\n",
    "\n",
    "_RETRIABLE_ERRNOS = {\n",
    "    110,  # Connection timed out [socket.py]\n",
    "}\n",
    "\n",
    "\n",
    "def _is_retriable(e):\n",
    "  return isinstance(e, IOError) and e.errno in _RETRIABLE_ERRNOS\n",
    "\n",
    "\n",
    "@retry(initial_delay=1.0, max_delay=16.0, is_retriable=_is_retriable)\n",
    "def urlretrieve_with_retry(url, filename=None):\n",
    "  return urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "\n",
    "def maybe_download(filename, work_directory, source_url):\n",
    "  \"\"\"Download the data from source url, unless it's already here.\n",
    "  Args:\n",
    "      filename: string, name of the file in the directory.\n",
    "      work_directory: string, path to working directory.\n",
    "      source_url: url to download from if file doesn't exist.\n",
    "  Returns:\n",
    "      Path to resulting file.\n",
    "  \"\"\"\n",
    "  if not gfile.Exists(work_directory):\n",
    "    gfile.MakeDirs(work_directory)\n",
    "  filepath = os.path.join(work_directory, filename)\n",
    "  if not gfile.Exists(filepath):\n",
    "    temp_file_name, _ = urlretrieve_with_retry(source_url)\n",
    "    gfile.Copy(temp_file_name, filepath)\n",
    "    with gfile.GFile(filepath) as f:\n",
    "      size = f.size()\n",
    "    print('Successfully downloaded', filename, size, 'bytes.')\n",
    "  return filepath\n",
    "\n",
    "def _read32(bytestream):\n",
    "  dt = numpy.dtype(numpy.uint32).newbyteorder('>')\n",
    "  return numpy.frombuffer(bytestream.read(4), dtype=dt)[0]\n",
    "\n",
    "\n",
    "def extract_images(f):\n",
    "  \"\"\"Extract the images into a 4D uint8 numpy array [index, y, x, depth].\n",
    "  Args:\n",
    "    f: A file object that can be passed into a gzip reader.\n",
    "  Returns:\n",
    "    data: A 4D uint8 numpy array [index, y, x, depth].\n",
    "  Raises:\n",
    "    ValueError: If the bytestream does not start with 2051.\n",
    "  \"\"\"\n",
    "  print('Extracting', f.name)\n",
    "  with gzip.GzipFile(fileobj=f) as bytestream:\n",
    "    magic = _read32(bytestream)\n",
    "    if magic != 2051:\n",
    "      raise ValueError('Invalid magic number %d in MNIST image file: %s' %\n",
    "                       (magic, f.name))\n",
    "    num_images = _read32(bytestream)\n",
    "    rows = _read32(bytestream)\n",
    "    cols = _read32(bytestream)\n",
    "    buf = bytestream.read(rows * cols * num_images)\n",
    "    data = numpy.frombuffer(buf, dtype=numpy.uint8)\n",
    "    data = data.reshape(num_images, rows, cols, 1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "  \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "  num_labels = labels_dense.shape[0]\n",
    "  index_offset = numpy.arange(num_labels) * num_classes\n",
    "  labels_one_hot = numpy.zeros((num_labels, num_classes))\n",
    "  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "  return labels_one_hot\n",
    "\n",
    "\n",
    "def extract_labels(f, one_hot=False, num_classes=10):\n",
    "  \"\"\"Extract the labels into a 1D uint8 numpy array [index].\n",
    "  Args:\n",
    "    f: A file object that can be passed into a gzip reader.\n",
    "    one_hot: Does one hot encoding for the result.\n",
    "    num_classes: Number of classes for the one hot encoding.\n",
    "  Returns:\n",
    "    labels: a 1D uint8 numpy array.\n",
    "  Raises:\n",
    "    ValueError: If the bystream doesn't start with 2049.\n",
    "  \"\"\"\n",
    "  print('Extracting', f.name)\n",
    "  with gzip.GzipFile(fileobj=f) as bytestream:\n",
    "    magic = _read32(bytestream)\n",
    "    if magic != 2049:\n",
    "      raise ValueError('Invalid magic number %d in MNIST label file: %s' %\n",
    "                       (magic, f.name))\n",
    "    num_items = _read32(bytestream)\n",
    "    buf = bytestream.read(num_items)\n",
    "    labels = numpy.frombuffer(buf, dtype=numpy.uint8)\n",
    "    if one_hot:\n",
    "      return dense_to_one_hot(labels, num_classes)\n",
    "    return labels\n",
    "\n",
    "\n",
    "class DataSet(object):\n",
    "\n",
    "  def __init__(self,\n",
    "               images,\n",
    "               labels,\n",
    "               fake_data=False,\n",
    "               one_hot=False,\n",
    "               dtype=dtypes.float32,\n",
    "               reshape=True,\n",
    "               seed=None):\n",
    "    \"\"\"Construct a DataSet.\n",
    "    one_hot arg is used only if fake_data is true.  `dtype` can be either\n",
    "    `uint8` to leave the input as `[0, 255]`, or `float32` to rescale into\n",
    "    `[0, 1]`.  Seed arg provides for convenient deterministic testing.\n",
    "    \"\"\"\n",
    "    seed1, seed2 = random_seed.get_seed(seed)\n",
    "    # If op level seed is not set, use whatever graph level seed is returned\n",
    "    numpy.random.seed(seed1 if seed is None else seed2)\n",
    "    dtype = dtypes.as_dtype(dtype).base_dtype\n",
    "    if dtype not in (dtypes.uint8, dtypes.float32):\n",
    "      raise TypeError(\n",
    "          'Invalid image dtype %r, expected uint8 or float32' % dtype)\n",
    "    if fake_data:\n",
    "      self._num_examples = 10000\n",
    "      self.one_hot = one_hot\n",
    "    else:\n",
    "      assert images.shape[0] == labels.shape[0], (\n",
    "          'images.shape: %s labels.shape: %s' % (images.shape, labels.shape))\n",
    "      self._num_examples = images.shape[0]\n",
    "\n",
    "      # Convert shape from [num examples, rows, columns, depth]\n",
    "      # to [num examples, rows*columns] (assuming depth == 1)\n",
    "      if reshape:\n",
    "        assert images.shape[3] == 1\n",
    "        images = images.reshape(images.shape[0],\n",
    "                                images.shape[1] * images.shape[2])\n",
    "      if dtype == dtypes.float32:\n",
    "        # Convert from [0, 255] -> [0.0, 1.0].\n",
    "        images = images.astype(numpy.float32)\n",
    "        images = numpy.multiply(images, 1.0 / 255.0)\n",
    "    self._images = images\n",
    "    self._labels = labels\n",
    "    self._epochs_completed = 0\n",
    "    self._index_in_epoch = 0\n",
    "\n",
    "  @property\n",
    "  def images(self):\n",
    "    return self._images\n",
    "\n",
    "  @property\n",
    "  def labels(self):\n",
    "    return self._labels\n",
    "\n",
    "  @property\n",
    "  def num_examples(self):\n",
    "    return self._num_examples\n",
    "\n",
    "  @property\n",
    "  def epochs_completed(self):\n",
    "    return self._epochs_completed\n",
    "\n",
    "  def next_batch(self, batch_size, fake_data=False, shuffle=True):\n",
    "    \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "    if fake_data:\n",
    "      fake_image = [1] * 784\n",
    "      if self.one_hot:\n",
    "        fake_label = [1] + [0] * 9\n",
    "      else:\n",
    "        fake_label = 0\n",
    "      return [fake_image for _ in xrange(batch_size)], [\n",
    "          fake_label for _ in xrange(batch_size)\n",
    "      ]\n",
    "    start = self._index_in_epoch\n",
    "    # Shuffle for the first epoch\n",
    "    if self._epochs_completed == 0 and start == 0 and shuffle:\n",
    "      perm0 = numpy.arange(self._num_examples)\n",
    "      numpy.random.shuffle(perm0)\n",
    "      self._images = self.images[perm0]\n",
    "      self._labels = self.labels[perm0]\n",
    "    # Go to the next epoch\n",
    "    if start + batch_size > self._num_examples:\n",
    "      # Finished epoch\n",
    "      self._epochs_completed += 1\n",
    "      # Get the rest examples in this epoch\n",
    "      rest_num_examples = self._num_examples - start\n",
    "      images_rest_part = self._images[start:self._num_examples]\n",
    "      labels_rest_part = self._labels[start:self._num_examples]\n",
    "      # Shuffle the data\n",
    "      if shuffle:\n",
    "        perm = numpy.arange(self._num_examples)\n",
    "        numpy.random.shuffle(perm)\n",
    "        self._images = self.images[perm]\n",
    "        self._labels = self.labels[perm]\n",
    "      # Start next epoch\n",
    "      start = 0\n",
    "      self._index_in_epoch = batch_size - rest_num_examples\n",
    "      end = self._index_in_epoch\n",
    "      images_new_part = self._images[start:end]\n",
    "      labels_new_part = self._labels[start:end]\n",
    "      return numpy.concatenate(\n",
    "          (images_rest_part, images_new_part), axis=0), numpy.concatenate(\n",
    "              (labels_rest_part, labels_new_part), axis=0)\n",
    "    else:\n",
    "      self._index_in_epoch += batch_size\n",
    "      end = self._index_in_epoch\n",
    "      return self._images[start:end], self._labels[start:end]\n",
    "\n",
    "\n",
    "def read_data_sets(train_dir,\n",
    "                   fake_data=False,\n",
    "                   one_hot=False,\n",
    "                   dtype=dtypes.float32,\n",
    "                   reshape=True,\n",
    "                   validation_size=5000,\n",
    "                   seed=None,\n",
    "                   source_url=DEFAULT_SOURCE_URL):\n",
    "  if fake_data:\n",
    "\n",
    "    def fake():\n",
    "      return DataSet(\n",
    "          [], [], fake_data=True, one_hot=one_hot, dtype=dtype, seed=seed)\n",
    "\n",
    "    train = fake()\n",
    "    validation = fake()\n",
    "    test = fake()\n",
    "    return Datasets(train=train, validation=validation, test=test)\n",
    "\n",
    "  if not source_url:  # empty string check\n",
    "    source_url = DEFAULT_SOURCE_URL\n",
    "\n",
    "  TRAIN_IMAGES = 'train-images-idx3-ubyte.gz'\n",
    "  TRAIN_LABELS = 'train-labels-idx1-ubyte.gz'\n",
    "  TEST_IMAGES = 't10k-images-idx3-ubyte.gz'\n",
    "  TEST_LABELS = 't10k-labels-idx1-ubyte.gz'\n",
    "\n",
    "  local_file = maybe_download(TRAIN_IMAGES, train_dir,\n",
    "                                   source_url + TRAIN_IMAGES)\n",
    "  with gfile.Open(local_file, 'rb') as f:\n",
    "    train_images = extract_images(f)\n",
    "\n",
    "  local_file = maybe_download(TRAIN_LABELS, train_dir,\n",
    "                                   source_url + TRAIN_LABELS)\n",
    "  with gfile.Open(local_file, 'rb') as f:\n",
    "    train_labels = extract_labels(f, one_hot=one_hot)\n",
    "\n",
    "  local_file = maybe_download(TEST_IMAGES, train_dir,\n",
    "                                   source_url + TEST_IMAGES)\n",
    "  with gfile.Open(local_file, 'rb') as f:\n",
    "    test_images = extract_images(f)\n",
    "\n",
    "  local_file = maybe_download(TEST_LABELS, train_dir,\n",
    "                                   source_url + TEST_LABELS)\n",
    "  with gfile.Open(local_file, 'rb') as f:\n",
    "    test_labels = extract_labels(f, one_hot=one_hot)\n",
    "\n",
    "  if not 0 <= validation_size <= len(train_images):\n",
    "    raise ValueError('Validation size should be between 0 and {}. Received: {}.'\n",
    "                     .format(len(train_images), validation_size))\n",
    "\n",
    "  validation_images = train_images[:validation_size]\n",
    "  validation_labels = train_labels[:validation_size]\n",
    "  train_images = train_images[validation_size:]\n",
    "  train_labels = train_labels[validation_size:]\n",
    "\n",
    "  options = dict(dtype=dtype, reshape=reshape, seed=seed)\n",
    "\n",
    "  train = DataSet(train_images, train_labels, **options)\n",
    "  validation = DataSet(validation_images, validation_labels, **options)\n",
    "  test = DataSet(test_images, test_labels, **options)\n",
    "\n",
    "  return Datasets(train=train, validation=validation, test=test)\n",
    "\n",
    "\n",
    "def load_mnist(train_dir='MNIST-data'):\n",
    "  return read_data_sets(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutaj zaczynamy właściwą pracę - import bibliotek i bazy danych\n",
    "Jak wyżej, ściągamy istotne dla nas biblioteki i odpalamy bazę ściągniętą z archiwów internetu\n",
    "wysłużoną bazę MINST. Zwróć uwagę na komentarze. Postarałem się w tej sekcji wyjaśnić możliwie dużo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Ściągamy biblioteki operacyjne. Jak nie wiesz co to jest cofnij się do poprzednich zeszytów kursowych.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Poniższy kod ściąga bazę MNIST i przyporządkowuje ją do zmiennej.\n",
    "# Jest to duże ułatwienie na poziomie szkolenia względem tego co robiono kiedyś, \n",
    "# a co możesz sobie obejrzeć w kodzie działającym właściwie identycznie w folderze \"spyder\"\n",
    "# od którego rozpocząłem swoją analizę. Z czasem społeczność stworzyła szereg ułatwień\n",
    "# i paczka startowa samouczka jest przewidziana jako opcjonalny download ze strony twórcy.\n",
    "# Dane są już preprocesowane, oraz rozbite na treningowe, walidacyjne i testowe, \n",
    "# zatem i o to nie musisz się tym razem martwić. \n",
    "# PRZESTARZAŁY KOD: from tensorflow.examples.tutorials.mnist import input_data\n",
    "# UWAGA: Do powyższego kodu zastosowano obejście stworzone przez twórców TF. (Ta wielka kobyła powyżej)\n",
    "# Używanie go pomimo ostrzeżeń o wycofywaniu grozi błędem i w ogóle... :P\n",
    "\n",
    "# Read_data_sets ma dwa argumenty. Pierwszy mówi funkcji o lokalizacji danych a drugi o sposobie\n",
    "# przyporządkowywania (kodowania pozycji) zmiennych. One_hot jest dosyć popularne, gdyż sprawdza się przy\n",
    "# niewielkiej liczbie argumentów i nie generuje niepotrzebnych korelacji między zmiennymi.\n",
    "# Jeśli jednak masz dużo zmiennych, to warto zastosować binary (czyli one_hot=\"False\"), gdyż posiadanie\n",
    "# np. kilkuset, lub kilkunastu tysięcy zmiennych kodujących pozycję będzie dość zasobożerne...\n",
    "mnist = read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Rysujemy i trenujemy model\n",
    "Tak jak w poprzednich przykładach. Możesz porównać z zeszytami 1 i 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/SpookyProgrammer/anaconda3/envs/tensorenv/lib/python3.6/site-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoka: 1. Strata z Treningu : 0.434. Strata z Walidacji: 0.208. Trafność Walidacji: 93.96%\n",
      "Epoka: 2. Strata z Treningu : 0.196. Strata z Walidacji: 0.163. Trafność Walidacji: 95.30%\n",
      "Epoka: 3. Strata z Treningu : 0.148. Strata z Walidacji: 0.134. Trafność Walidacji: 96.08%\n",
      "Epoka: 4. Strata z Treningu : 0.118. Strata z Walidacji: 0.129. Trafność Walidacji: 96.18%\n",
      "Epoka: 5. Strata z Treningu : 0.098. Strata z Walidacji: 0.105. Trafność Walidacji: 96.82%\n",
      "Epoka: 6. Strata z Treningu : 0.083. Strata z Walidacji: 0.099. Trafność Walidacji: 97.06%\n",
      "Epoka: 7. Strata z Treningu : 0.071. Strata z Walidacji: 0.095. Trafność Walidacji: 97.16%\n",
      "Epoka: 8. Strata z Treningu : 0.064. Strata z Walidacji: 0.094. Trafność Walidacji: 97.12%\n",
      "Epoka: 9. Strata z Treningu : 0.055. Strata z Walidacji: 0.097. Trafność Walidacji: 97.12%\n",
      "Koniec Sesji Treningowej\n"
     ]
    }
   ],
   "source": [
    "### PANEL GŁÓWNY ###\n",
    "lr = 0.001              # Prędkość uczenia się modelu. Domyślna: 0.001\n",
    "bs = 100                # Wielkość partii w jakich uczymy model (patrz linijka 93 - batch_size)\n",
    "n_epochs = 15           # Górne organiczenie ilości \"epok\", tj. przejść przez bazę danych modelu (linijka 101 - max_epochs)\n",
    "input_size = 784        # Rozmiar danych wejściowych to 784, gdyż rozmiar obrazka w bazie danych MNIST to 28x28px=784px.\n",
    "output_size = 10        # Rozmiar danych wyjściowych to 10, gdyż baza danych MNIST zawiera 10 cyfr od 0 do 9\n",
    "                        # z których model wybiera właściwe odpowiedzi z pewnym prawdopodobieństwem dla każdej.\n",
    "hidden_layer_size = 50  # rozmiar każdej ukrytej warstwy \"neuronów\".\n",
    "# Założyliśmy tutaj dość często używaną metodykę, że wszystkie ukryte warstwy mają tę samą \"grubość\". Domyślna: 50.\n",
    "# W zależności jednak od problemu, który chcesz rozwiązać, możesz używać warstw różnej wielkości. Spróbuj np. 100.\n",
    "\n",
    "tf.reset_default_graph() # Resetuje zmienne zapisane w pamięci modelu z jego poprzednich przebiegów.\n",
    "# Jest to o tyle konieczne, że deklarujemy nasze zmienne jako globalne jako część domyślnego (default) modelu,\n",
    "# i niewykonanie tej operacji może nam namieszać, kiedy dopracowujemy model w ramach eksperymentów.\n",
    "\n",
    "# Podstawniki jak w zeszycie 2.\n",
    "inputs = tf.placeholder(tf.float32, [None, input_size])\n",
    "targets = tf.placeholder(tf.float32, [None, output_size])\n",
    "\n",
    "# Używamy metody tf.get_variable(\"nazwa_zmiennej\",[kształt,zmiennej]) aby zdefiniować Wagi i Obciążenia\n",
    "# dla każdego połączenia. W tym wypadku jest to np. Wagi_1 (z wejściowych do 1 ukrytej warstwy) =\n",
    "# = tf.get_variable(\"wagi_1\", [rozmiar_danych_wejściowych np. 784, rozmiar_warstwy ukrytej np. 50])\n",
    "weights_1 = tf.get_variable(\"weights_1\", [input_size, hidden_layer_size])\n",
    "biases_1 = tf.get_variable(\"biases_1\", [hidden_layer_size])\n",
    "# outputs_1 to wynik aktywacji w ramach wybranej przez nas funkcji aktywacji (tutaj wybralismy dość popularną Relu)\n",
    "# Możesz wypróbować inne, jak sigmoid, ale tutaj raczej zobaczysz, że będzie to skutkowało powolnym uczeniem się.\n",
    "outputs_1 = tf.nn.relu(tf.matmul(inputs, weights_1) + biases_1)\n",
    "\n",
    "# Druga ukryta warstwa ma rozmiar ukryta_warstwa x ukryta warstwa, co jest logiczne, czyż nie? ;)\n",
    "# Wszystkie kolejne ukryte warstwy będą miały taki rozmiar, poza ostatnią, którą będziemy \"mnożyć\"\n",
    "# przez rozmiar danych wyjściowych.\n",
    "weights_2 = tf.get_variable(\"weights_2\",[hidden_layer_size, hidden_layer_size])\n",
    "biases_2 = tf.get_variable(\"biases_2\",[hidden_layer_size])\n",
    "# Zwróć uwagę z czego zwracamy wynik outputs_2, czyli wynik aktywacji drugiej warstwy.\n",
    "outputs_2 = tf.nn.relu(tf.matmul(outputs_1, weights_2) + biases_2)\n",
    "\n",
    "# W tym przykładzie używam dwóch ukrytych warstw, mimo, że jest to ilość suboptymalna, \n",
    "# po prostu, żeby pokazać jak działają. Możesz zmienne dodawać ręcznie, \n",
    "# albo, do czego zachęcam, zrobić sobie funkcję, która będzie dodawać (albo co lepsze recyclingować jak u Hindusa) \n",
    "# kolejne warstwy za Ciebie.\n",
    "\n",
    "# Wagi łączące ostatnią warstwę ukrytą z warstwą danych wyjściowych (outputs).\n",
    "weights_3 = tf.get_variable(\"weights_3\", [hidden_layer_size, output_size])\n",
    "biases_3 = tf.get_variable(\"biases_3\", [output_size])\n",
    "\n",
    "# Zwróć uwagę, że nie zwróciliśmy tutaj od razu funkcji aktywacyjnej, a jedynie iloczyn skalarny + obciążenie 3.\n",
    "\n",
    "outputs = tf.matmul(outputs_2, weights_3) + biases_3\n",
    "\n",
    "# Poniższa funkcja aplikuje zarówno Aktywator Softmax, który jest wskazany przy danych wyjściowych\n",
    "# (bo daje wyskalowane przedziały ufności dzielone przez dostępne opcje sumarycznie do 1)\n",
    "# oraz aplikuje koszt z funkcji logarytmicznej liczony od całkowitego błędu. \n",
    "# Format użycia to: tf.nn.softmax_cross_entropy_with_logits(logits,labels) Tutaj: logits = wyjściowe, labels = cele (targets)\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=outputs, labels=targets)\n",
    "\n",
    "# Poniższa metoda poprawia nam osiągi. W tej sposób TF szuka nam średnią elementów tensora w danym wymiarze.\n",
    "# Więcej o zastosowaniu ENG tutaj*\n",
    "mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "# Wybieramy model treningowy. Ponieważ poszliśmy do przodu z teorią, to tym razem wybierzemy sobie ADAM,\n",
    "# który jest o tyle dobry, że raczej nie zacina się na lokalnym (fałszywym) minimum funkcji i optymalnie szuka\n",
    "# globalnego minimum. Jest to metoda z 2015, więc dość nowa. Daj mi znać jeśli wymyślono coś lepszego.\n",
    "optimize = tf.train.AdamOptimizer(learning_rate=lr).minimize(mean_loss)\n",
    "\n",
    "# Metoda tf.argmax pozwala nam wybrać która zmienna ma najwyższy \"wynik\" (patrz niżej) zwracany przez funkcję w sposób: \n",
    "# (outputs: z aktywatora softmax, tj. tutaj model próbuje \"zgadnąć\" prawidłowy wynik o który nam chodzi,\n",
    "# przydzielając poszczególnym zmiennym prawdopodobieństwo. Agrmax wybiera najwyższe prawdopodobieństwo.)\n",
    "# (targets: zdefiniowane przez nas w ramach bazy \"labels\", do czego dąży model. \n",
    "# W wypadku \"OneHot\" tylko jedna zmienna będzie miała wartość 1 a pozostałe 0 i to ją wybierze Argmax.)\n",
    "# tf.equal (bool) porównuje te dwa ciągi, które zwracają argmax i jeśli się pokrywają zwraca 1 a jeśli nie 0.\n",
    "# Otrzymujemy więc wektor zero-jedynkowy, który nabierze sensu w ramach kolejnej funkcji...\n",
    "out_equals_target = tf.equal(tf.argmax(outputs,1), tf.argmax(targets,1))\n",
    "\n",
    "# tf.reduce_mean*, o którym możesz poczytać poniżej pozwala nam logicznie obliczyć trafność modelu.\n",
    "# Po prostu bierze wszystkie te 0 (nietrafione) i 1 (trafione decyzje modelu) i wyciąga średnią. Oto nasza trafność.:)\n",
    "# tf.cast(obiekt_do_konwersji, docelowy_format) to nasze ubezpieczenie, na wszelki wypadek\n",
    "# aby dane, które zwróci nam funkcja na pewno były w odpowiednim formacie. (tutaj tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))\n",
    "\n",
    "# Odpalamy sesję i inicjalizujemy zmienne w ramach sesji. Szczegóły w zeszycie 2, pkt 5 i 6.\n",
    "sess = tf.InteractiveSession()\n",
    "initializer = tf.global_variables_initializer()\n",
    "sess.run(initializer)\n",
    "\n",
    "# Wielkość pakietów. Zdecyduj o wielkości pakietów na które będzie dzielona baza treningowa.\n",
    "# Tzn. jak często model będzie poprawiał swoje wagi i obciążenia.\n",
    "# Jeśli wpiszesz ilość równą ilości obserwacji (niezalecane)\n",
    "# model przejdzie w ramach 1 epoki na raz przez całą bazę danych i będzie najdokładniejszy \n",
    "# (przykład: \"Gradient Descent\"), ale jest to bardzo zasobochłonne\n",
    "# Im mniejszy pakiet względem bazy danych, tym częstsza aktualizacja w ramach przechodzenia przez nią,\n",
    "# ale mniejsza dokładność (Jak w Stochastic Gradient Descent).\n",
    "# Ogólnie warto się z tym pobawić i raczej na ogół warto poświęcić trochę dokładności na rzecz szybkości.\n",
    "# Startowa : 100 mówi, że w ramach jednego \"przejścia\" przez bazę, \n",
    "# wagi i średnie zostaną dostosowane (ilość_przykładów / 100 razy). \n",
    "# Dzielenie \"floordiv\" zaokrągla ilość do równej wartości. // Edit: Wyciągnięta na panel główny.\n",
    "batch_size = bs\n",
    "# Ta zmienna to wynik dzielenia liczby przykładów treningowych w bazie danych przez rozmiar partii.\n",
    "batches_number = mnist.train._num_examples // batch_size\n",
    "\n",
    "# Poniżej dwa mechanizmy zatrzymujące:\n",
    "# Ta zmienna ogranicza z góry liczbę \"przejść\" przez bazę, tak aby model się nie zaciął. // Edit: Wyciągnięta na panel główny.\n",
    "max_epochs = n_epochs\n",
    "# Ta zmienna da nam pewność, że model nie przerwie działania po pierwszym przebiegu (epoce).\n",
    "# Model porównuje wyniki treningowe do bazy walidacyjnej i sprawdza, czy nie zaszło tzw. overfitting (naddopasowanie modelu)\n",
    "# Wysoka wartość poprzedniej straty z walidacj przeciwdziała tutaj przerwaniu po pierwszej epoce,\n",
    "# gdyż na początku model takowej wartości nie ma, więc domyślne 0 spowodowałoby niewłaściwe przerwanie, co mija się z celem...\n",
    "prev_validation_loss = 9999999.\n",
    "\n",
    "# Podobną pętlę masz w zeszycie 2. Jak coś jest niejasne sprawdź tam, bo staram się nie powtarzać.\n",
    "# W tym miejscu odbywa się uczenie modelu. Zauważ, że ta pętla nie przebiegnie więcej niż 15 razy.\n",
    "for epoch_counter in range(max_epochs):\n",
    "    \n",
    "    # Ustalamy zmienną do której pętla zapisuje obecną wartość funkcji straty.\n",
    "    curr_epoch_loss = 0.\n",
    "    \n",
    "    # Pętla kręci się do wykorzystania wszystkich partii.\n",
    "    for batch_counter in range(batches_number):\n",
    "        \n",
    "        # Tutaj pętla ładuje [100] wejść (input_batch) i [100] celów (target_batch) ([100] wynika z \"batch_size\")\n",
    "        input_batch, target_batch = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        # Tutaj optymalizuje algorytm i kalkuluje funkcję straty dla każdej partii. \n",
    "        _, batch_loss = sess.run([optimize, mean_loss], \n",
    "            feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        \n",
    "        # Tutaj zapisuje stratę dla obecnej iteracji pętli\n",
    "        curr_epoch_loss += batch_loss\n",
    "    \n",
    "    # Teraz zmienna curr_epoch_loss faktycznie zawiera średnią wartość funkcji straty dla zestawu treningowego\n",
    "    # dla kolejnej epoki. Otrzymujemy ją dzieląc sumę strat ze wszystkich partii w epoce przez liczbę partii.\n",
    "    curr_epoch_loss /= batches_number\n",
    "\n",
    "    # Tutaj ładujemy zestaw walidacyjny wyodrębiony już dla nas w paczce MNIST. Ten zestaw wykorzystujemy\n",
    "    # wielokrotnie, i podlega on jedynie propagacji w przód, tzn. nie dostosowujemy do niego wag modelu\n",
    "    # (nie trenujemy na nim). Dzieje się tak dlatego, że względem tej małej próbki (zwykle ok 10% bd)\n",
    "    # sprawdzamy, czy nie zaszło naddopasowanie (overfitting).\n",
    "    input_batch, target_batch = mnist.validation.next_batch(mnist.validation._num_examples)\n",
    "    \n",
    "    \n",
    "    # Oblicza średnią stratę i trafność zestawu walidacyjnego i zapisuje je w odpowiadających im zmiennych\n",
    "    validation_loss, validation_accuracy = sess.run([mean_loss, accuracy], \n",
    "                                                    feed_dict={inputs: input_batch, targets: target_batch})\n",
    "    \n",
    "    # Zwraca nam w widoczny sposób wartości zmiennych dla każdej 'Epoki'. Formatowanie typowo pythonowskie.\n",
    "    print('Epoka: '+str(epoch_counter+1)+\n",
    "          '. Strata z Treningu : '+'{0:.3f}'.format(curr_epoch_loss)+\n",
    "          '. Strata z Walidacji: '+'{0:.3f}'.format(validation_loss)+\n",
    "          '. Trafność Walidacji: '+'{0:.2f}'.format(validation_accuracy * 100)+'%')\n",
    "    \n",
    "    # Przerywa pętlę wcześniej jeśli strata z walidacji zacznie wzrastać wskazując na naddopasowanie modelu.\n",
    "    if validation_loss > prev_validation_loss:\n",
    "        break\n",
    "        \n",
    "    # Aktualizuje stratę z walidacji, żeby ją można było porównać po każdej epoce.    \n",
    "    prev_validation_loss = validation_loss\n",
    "        \n",
    "print(\"Koniec Sesji Treningowej\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Metoda tf.reduce_mean - przykłady: \n",
    "https://stackoverflow.com/questions/34236252/difference-between-np-mean-and-tf-reduce-mean-in-numpy-and-tensorflow#47242212"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Testujemy Model\n",
    "Baza testowa, to baza, której nasz model nigdy nie \"widział\", w przeciwieństwie do bazy walidacyjnej,która pomagała nam uniknąć \"naddopasowania\". Teraz na takiej \"dziewiczej\" bazie danych poznamy bliższą prawdziwej wartość Trafności (accuracy) naszego modelu. Czy i na ile mierzy to co miał mierzyć? Przekonajmy się sami. ;)\n",
    "Wynik może się poprawić lub pogorszyć, ale na ten moment jest to wynik ostateczny.\n",
    "\n",
    "### UWAGA!!!\n",
    "Pamiętaj, że jak już raz odpalisz model na bazie testowej, to (zgodnie ze sztuką) nie należy już ponownie poprawiać modelu (dłubać przy hiperparametrach - patrz: model główny). Ta wartość trafności powinna być uznana za ostateczną, bo inaczej (logicznie rzecz biorąc) naddopasowujesz model do bazy testowej, co mija się z celem.\n",
    "\n",
    "W sytuacji, gdy dalej chcesz dłubać przy modelu, zdobądź nowe próbki do bazy testowej a istniejące wkomponuj w istniejący model (do bazy treningowej/walidacyjnej). Oczywiście baza MNIST jest dla Ciebie bazą treningową, niemniej pamiętaj o tym istotnym założeniu badawczym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trafność Testowa: 97.17%\n"
     ]
    }
   ],
   "source": [
    "# Kod analogiczny do kodu ładującego i przypisującego zmienne w części 1 do zestawu walidacyjnego,\n",
    "# więc nie będę się rozpisywał...\n",
    "input_batch, target_batch = mnist.test.next_batch(mnist.test._num_examples)\n",
    "test_accuracy = sess.run([accuracy], \n",
    "    feed_dict={inputs: input_batch, targets: target_batch})\n",
    "\n",
    "# Zmienna test_accuracy, to lista o 1 wartości, z której wyciągamy tę wartość żądając pozycji [0].\n",
    "# print (test_accuracy)\n",
    "test_accuracy_percent = test_accuracy[0]\n",
    "\n",
    "# Formatujemy analogicznie do Trafności walidacji:\n",
    "print('Trafność Testowa: '+'{0:.2f}'.format(test_accuracy_percent * 100)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
