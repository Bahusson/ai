{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sekwencyjny model CNN z Keras i własną Bazą Danych\n",
    "W tym zeszycie używam własnej, przygotowanej wcześniej bazy obrazków do analizy.\n",
    "Obrazki mogą być w dowolnym formacie, ale z przyzwyczajenia użyłem takiego samego jak w MNIST.\n",
    "Tym razem jednak są w formacie PNG (mogą być i JPG), w skali szarości i ich procesowanie wyglądało nieco inaczej.\n",
    "Jak przygotować zbiór danych piszę poniżej:\n",
    "\n",
    "<Tu wstaw opis jak przygotowałeś zbiór danych jak już kod będzie działał poprawnie>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Poniższy model daje nam jako takie pojęcie o tym w jaki sposób można przygotować niewielką ilość danych tak,\n",
    "# aby model jednak się czegoś nauczył. W końcu nie zawsze mamy jakieś pół miliona rekordów. Ja dostałem powalającą\n",
    "# ilość 150 case'ów ćwiczebnych, także trzeba było kombinować. Poniżej masz rezultat mojego bezczelnego zżyniania,\n",
    "# optymalizowania i kombinowania. Enjoy. :P\n",
    "\n",
    "# Oryginalny kod pożyczony w słusznej sprawie z: https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "#from keras.optimizers import SGD #Używam Adam, ale mogę chcieć użyć np. SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Podajemy wymiary obrazka. Ja używam formatu takiego jak w MNIST, ale możesz mieć każdy inny...\n",
    "img_width, img_height = 28, 28\n",
    "\n",
    "num_classes = 10 # Ilość kategorii\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 150\n",
    "nb_validation_samples = 80\n",
    "epochs = 15\n",
    "batch_size = 10\n",
    "color_cha = 1 # Ilość kanałów. W Odcieniach szarości będziesz miał 1, W kolorze (RGB) będziesz miał 3.\n",
    "\n",
    "# Ten fragment odczytuje z obrazków w jakim formacie są zapisane kanały kodowania kolorów.\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (color_cha, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, color_cha)\n",
    "\n",
    "# Część główna modelu. W porównaniu do TF, Keras jest bardzo prosty. \n",
    "# Jak klocki lego układa się warstwa na warstwie.\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), # Tutaj kernel = filtr. Tj. 'przesuwamy' tym naszym filtrem o kształcie 3x3 po obrazku.\n",
    "                                         # Poczytaj albo najlepiej obejrzyj film o CNN, bo to trudno opisać słowami.:)\n",
    "                                         # Sprowadza się to do wyciągnięcia z tego 'przesuwania' kolejnej macierzy z Iloczynem Skalarnym.\n",
    "                                         # W ten sposób można znaleźć 'interesujące', tj. kluczowe elementy obrazka i zaoszczędzić\n",
    "                                         # na mocy obliczeniowej, oraz godzinach pracy przy ręcznym obrabianiu baz danych... Ale naprawdę obejrzyj wizualizację. Warto.\n",
    "                 activation='relu',  # Wszyscy raczej zgadzają się, że Relu w warstwach ukrytych jest najpopularniejszy do aktywacji, bo przyspiesza uczenie.\n",
    "                 input_shape=input_shape))\n",
    "# Tutaj zauważ zmianę względem poprzedniego modelu.\n",
    "model.add(Conv2D(64, (3, 3), activation='relu')) # W następnym już nie definiuję słownie kernel_size = (x,y), ale wiadomo ocb.\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25)) # Pokombinuj z wartościami 'dropout'. Raczej zaczynaj od mniejszych i zwiększaj z czasem.\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5)) # Nie dawaj wyższych wartości końcowych 'dropout' niż 0.5, chyba, że wiesz co robisz...\n",
    "# Sposób aktywacji ostatniej warstwy inny niż na przykładzie wyjściowym, \n",
    "# bo mam 10 kategorii, więc lepiej wziąć softmax.\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "                    # Sposób kalkulowania straty. Inny niż w przykładzie z którego brałem kod, bo mam aż 10 kategorii.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              #Tutaj warto poeksperymentować z metodą optymalizacji:\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 150 images belonging to 10 classes.\n",
      "Found 80 images belonging to 10 classes.\n",
      "Epoch 1/15\n",
      "15/15 [==============================] - 5s 338ms/step - loss: 2.2558 - acc: 0.1917 - val_loss: 1.9656 - val_acc: 0.5134\n",
      "Epoch 2/15\n",
      "15/15 [==============================] - 3s 199ms/step - loss: 1.6991 - acc: 0.4901 - val_loss: 1.0322 - val_acc: 0.7837\n",
      "Epoch 3/15\n",
      "15/15 [==============================] - 3s 201ms/step - loss: 1.1209 - acc: 0.6028 - val_loss: 0.5652 - val_acc: 0.8894\n",
      "Epoch 4/15\n",
      "15/15 [==============================] - 3s 202ms/step - loss: 0.7953 - acc: 0.7306 - val_loss: 0.3106 - val_acc: 0.9196\n",
      "Epoch 5/15\n",
      "15/15 [==============================] - 3s 210ms/step - loss: 0.5975 - acc: 0.8092 - val_loss: 0.2077 - val_acc: 0.9567\n",
      "Epoch 6/15\n",
      "15/15 [==============================] - 3s 204ms/step - loss: 0.5060 - acc: 0.8384 - val_loss: 0.1496 - val_acc: 0.9615\n",
      "Epoch 7/15\n",
      "15/15 [==============================] - 3s 201ms/step - loss: 0.4195 - acc: 0.8805 - val_loss: 0.0904 - val_acc: 1.0000\n",
      "Epoch 8/15\n",
      "15/15 [==============================] - 3s 198ms/step - loss: 0.2934 - acc: 0.8958 - val_loss: 0.0655 - val_acc: 0.9904\n",
      "Epoch 9/15\n",
      "15/15 [==============================] - 3s 204ms/step - loss: 0.3158 - acc: 0.8918 - val_loss: 0.0615 - val_acc: 0.9856\n",
      "Epoch 10/15\n",
      "15/15 [==============================] - 3s 213ms/step - loss: 0.2744 - acc: 0.9128 - val_loss: 0.0433 - val_acc: 1.0000\n",
      "Epoch 11/15\n",
      "15/15 [==============================] - 3s 199ms/step - loss: 0.2105 - acc: 0.9223 - val_loss: 0.0317 - val_acc: 1.0000\n",
      "Epoch 12/15\n",
      "15/15 [==============================] - 3s 200ms/step - loss: 0.1795 - acc: 0.9471 - val_loss: 0.0253 - val_acc: 1.0000\n",
      "Epoch 13/15\n",
      "15/15 [==============================] - 3s 201ms/step - loss: 0.2053 - acc: 0.9361 - val_loss: 0.0155 - val_acc: 1.0000\n",
      "Epoch 14/15\n",
      "15/15 [==============================] - 3s 201ms/step - loss: 0.1818 - acc: 0.9400 - val_loss: 0.0156 - val_acc: 1.0000\n",
      "Epoch 15/15\n",
      "15/15 [==============================] - 3s 208ms/step - loss: 0.1432 - acc: 0.9576 - val_loss: 0.0147 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "### Preprocesowanie obrazków z TF Keras: https://keras.io/preprocessing/image/ ###\n",
    "\n",
    "# Konfiguracja preprocesująca, której używamy do treningu.\n",
    "train_datagen = ImageDataGenerator(\n",
    "# obowiązkowe przeskalowanie (Normalizacja) z obowiązkowej skali int 0-225 na float32 0-1\n",
    "    rescale = 1. / 255,\n",
    "    rotation_range = 4, # W stopniach. Losowo obraca obrazki o tyle stopni.\n",
    "    # Przycięcie obrazka do wartości zbliżenia.\n",
    "     shear_range = 0.2,\n",
    "    # Powiększenie obrazka o wartość przycięcia.\n",
    "    zoom_range = 0.2,\n",
    "    )\n",
    "# Jakbyśmy mieli np. obrazki psów i kotów, albo samochodów, to można je jeszcze np. przerzucić w pionie. \n",
    "# W wypadku pewnych zbiorów danych jak np. litery, cyfry itp. znaki, to jest imho wątpliwe ;)\n",
    "# Kod przerzucający poniżej: horizontal_flip=True. Wrzuć wraz z resztą w nawias. \n",
    "# Więcej i pomocy tricków znajdziesz w Zakładce ImageDataGenerator w linku u góry.\n",
    "# Jak to odpowiednio rozbujać, to wygląda jakbyśmy chcieli zafundować maszynie halucynacje... :]\n",
    "\n",
    "# Konfiguracja preprocesująca, której użyjemy do walidacji i testów:\n",
    "# Tylko przeskalowanie nasycenia z int na float.\n",
    "test_datagen = ImageDataGenerator(rescale = 1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    color_mode='grayscale',  # Obrazki są w skali szarości. Jak tego nie damy, to model przyjmie, że są w RGB.\n",
    "    class_mode='categorical') # Typ klasyfikacji. Tutaj zwraca nam 10 typu one-hot.\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)\n",
    "    \n",
    "model.save_weights('first_try.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
